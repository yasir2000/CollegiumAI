"""
CollegiumAI - Comprehensive Integration and Testing Suite
Tests for cognitive architecture, multi-agent collaboration, and persona-specific intelligence
"""

import asyncio
import pytest
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
import sys
import os

# Add the project root to the path
sys.path.insert(0, str(Path(__file__).parent))

# Import CollegiumAI components
from framework.cognitive import (
    CognitiveEngine, PersonaCognitiveAgent, PersonaType, 
    CognitivePersonaFactory, DecisionContext, DecisionType,
    AttentionTarget, LearningType, MetacognitiveState
)
from multi_agent_system import MultiAgentOrchestrator, AutonomousAgent, AgentRole
from cognitive_architecture_demo import CognitiveArchitectureDemo

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Mock OllamaClient for testing
class MockOllamaClient:
    """Mock Ollama client for testing purposes"""
    
    async def generate_response(self, prompt: str, model: str = "llama2", **kwargs):
        # Return a mock response for testing
        return {
            "response": f"Mock response for: {prompt[:50]}...",
            "model": model,
            "done": True
        }
    
    async def chat(self, messages: list, model: str = "llama2", **kwargs):
        return {
            "message": {
                "role": "assistant",
                "content": f"Mock chat response to {len(messages)} messages"
            },
            "done": True
        }


class CollegiumAITestSuite:
    """
    Comprehensive test suite for CollegiumAI's cognitive architecture
    and multi-agent collaboration system
    """
    
    def __init__(self):
        self.test_results = []
        self.persona_factory = CognitivePersonaFactory()
        # Create mock client for testing
        mock_client = MockOllamaClient()
        self.multi_agent_system = MultiAgentOrchestrator(mock_client)
        
    async def run_all_tests(self):
        """Run comprehensive test suite"""
        
        print("üß™ CollegiumAI - Comprehensive Testing Suite")
        print("=" * 60)
        
        # Core cognitive architecture tests
        await self.test_cognitive_core()
        await self.test_perception_system()
        await self.test_reasoning_engine()
        await self.test_memory_system()
        await self.test_learning_system()
        await self.test_decision_making()
        await self.test_attention_mechanism()
        await self.test_metacognitive_controller()
        
        # Persona-specific tests
        await self.test_persona_creation()
        await self.test_persona_cognitive_profiles()
        await self.test_persona_request_processing()
        
        # Multi-agent collaboration tests
        await self.test_multi_agent_initialization()
        await self.test_agent_collaboration()
        await self.test_workflow_orchestration()
        
        # Integration tests
        await self.test_cognitive_integration()
        await self.test_end_to_end_scenarios()
        
        # Performance tests
        await self.test_performance_benchmarks()
        
        # Generate test report
        self.generate_test_report()
    
    async def test_cognitive_core(self):
        """Test core cognitive engine functionality"""
        
        print("\n1. üß† Testing Cognitive Core Engine")
        print("-" * 40)
        
        try:
            # Create cognitive engine
            engine = CognitiveEngine("test_persona")
            
            # Test cognitive cycle processing
            input_data = {
                "text": "I need help with my research methodology",
                "context": {"domain": "academic", "urgency": "medium"}
            }
            
            result = await engine.process_cognitive_cycle(input_data)
            
            # Validate results
            assert "cognitive_state" in result
            assert "action_plan" in result
            assert "confidence" in result
            assert result["confidence"] > 0
            
            self.test_results.append({
                "test": "Cognitive Core Engine",
                "status": "PASS",
                "details": {
                    "cognitive_cycle_complete": True,
                    "confidence": result["confidence"],
                    "processing_time": result.get("processing_time", 0)
                }
            })
            
            print("   ‚úÖ Cognitive cycle processing: PASS")
            print(f"   ‚úÖ Confidence score: {result['confidence']:.2f}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Cognitive Core Engine",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Cognitive Core Engine: FAIL - {e}")
    
    async def test_perception_system(self):
        """Test multi-modal perception system"""
        
        print("\n2. üëÅÔ∏è Testing Perception System")
        print("-" * 40)
        
        try:
            engine = CognitiveEngine("test_persona")
            
            # Test multi-modal input processing
            complex_input = {
                "text": "I'm stressed about my upcoming presentation and need help organizing my thoughts",
                "context": {
                    "emotional_cues": ["stress", "anxiety"],
                    "academic_domain": "presentation_skills",
                    "urgency": "high"
                }
            }
            
            result = await engine.perception_module.process_multi_modal_input(complex_input)
            
            # Validate perception results
            assert "salience_score" in result
            assert "emotional_patterns" in result
            assert "academic_patterns" in result
            assert result["salience_score"] > 0
            
            emotional_detected = "stress" in str(result.get("emotional_patterns", {})).lower()
            domain_detected = "presentation" in str(result.get("academic_patterns", {})).lower()
            
            self.test_results.append({
                "test": "Perception System",
                "status": "PASS",
                "details": {
                    "salience_score": result["salience_score"],
                    "emotional_detection": emotional_detected,
                    "domain_detection": domain_detected
                }
            })
            
            print("   ‚úÖ Multi-modal processing: PASS")
            print(f"   ‚úÖ Salience detection: {result['salience_score']:.2f}")
            print(f"   ‚úÖ Emotional patterns: {'Detected' if emotional_detected else 'Not detected'}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Perception System",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Perception System: FAIL - {e}")
    
    async def test_reasoning_engine(self):
        """Test reasoning capabilities"""
        
        print("\n3. üßÆ Testing Reasoning Engine")
        print("-" * 40)
        
        try:
            engine = CognitiveEngine("test_persona")
            
            # Test causal reasoning
            problem = "Student grades are declining over the semester"
            context = {"domain": "academic_performance", "factors": ["attendance", "engagement"]}
            
            causal_result = await engine.reasoning_engine.analyze_causal_relationships(problem, context)
            
            # Test analogical reasoning
            scenario = {"problem": problem, "context": context}
            analogical_result = await engine.reasoning_engine.apply_analogical_reasoning(
                scenario, {"domain": "education"}
            )
            
            # Validate reasoning results
            assert "potential_causes" in causal_result
            assert "insights" in analogical_result
            assert len(causal_result["potential_causes"]) > 0
            
            self.test_results.append({
                "test": "Reasoning Engine",
                "status": "PASS",
                "details": {
                    "causal_factors_identified": len(causal_result["potential_causes"]),
                    "analogical_insights": len(analogical_result["insights"]),
                    "overall_confidence": causal_result.get("overall_confidence", 0)
                }
            })
            
            print("   ‚úÖ Causal reasoning: PASS")
            print(f"   ‚úÖ Causal factors identified: {len(causal_result['potential_causes'])}")
            print("   ‚úÖ Analogical reasoning: PASS")
            
        except Exception as e:
            self.test_results.append({
                "test": "Reasoning Engine",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Reasoning Engine: FAIL - {e}")
    
    async def test_memory_system(self):
        """Test memory system functionality"""
        
        print("\n4. üß† Testing Memory System")
        print("-" * 40)
        
        try:
            engine = CognitiveEngine("test_persona")
            
            # Test memory storage
            memory_item = {
                "content": "Research methodology discussion with advisor",
                "context": {"domain": "research", "importance": 0.8},
                "timestamp": datetime.now()
            }
            
            await engine.memory_system.store_episodic_memory(memory_item)
            
            # Test memory retrieval
            query = {
                "query": "research methodology",
                "context": {"domain": "research"}
            }
            
            retrieval_result = await engine.memory_system.retrieve_relevant_memories(query)
            
            # Validate memory operations
            assert "episodic_memories" in retrieval_result
            assert "semantic_associations" in retrieval_result
            
            self.test_results.append({
                "test": "Memory System",
                "status": "PASS",
                "details": {
                    "storage_successful": True,
                    "retrieval_successful": True,
                    "memories_retrieved": len(retrieval_result.get("episodic_memories", []))
                }
            })
            
            print("   ‚úÖ Memory storage: PASS")
            print("   ‚úÖ Memory retrieval: PASS")
            print(f"   ‚úÖ Memories retrieved: {len(retrieval_result.get('episodic_memories', []))}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Memory System",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Memory System: FAIL - {e}")
    
    async def test_learning_system(self):
        """Test adaptive learning system"""
        
        print("\n5. üìö Testing Learning System")
        print("-" * 40)
        
        try:
            engine = CognitiveEngine("test_persona")
            
            # Test learning episode processing
            episode_data = {
                "input_data": "Learning about statistical analysis",
                "cognitive_state": {"motivation": 0.8, "cognitive_load": 0.6},
                "processing_time": 1.2,
                "confidence": 0.7
            }
            
            await engine.learning_systems["adaptive"].update_from_episode(episode_data)
            
            # Test learning strategy recommendation
            context = {"type": "statistics", "difficulty": 0.7}
            strategy = await engine.learning_systems["adaptive"].get_optimal_learning_strategy(context)
            
            # Validate learning system
            assert "recommended_modality" in strategy
            assert "current_learning_rate" in strategy
            
            self.test_results.append({
                "test": "Learning System",
                "status": "PASS",
                "details": {
                    "episode_processing": True,
                    "strategy_recommendation": True,
                    "recommended_modality": strategy["recommended_modality"]
                }
            })
            
            print("   ‚úÖ Learning episode processing: PASS")
            print("   ‚úÖ Strategy recommendation: PASS")
            print(f"   ‚úÖ Recommended modality: {strategy['recommended_modality']}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Learning System",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Learning System: FAIL - {e}")
    
    async def test_decision_making(self):
        """Test decision-making engine"""
        
        print("\n6. ‚öñÔ∏è Testing Decision Making Engine")
        print("-" * 40)
        
        try:
            engine = CognitiveEngine("test_persona")
            
            # Import decision making components
            from framework.cognitive.decision_making import DecisionCriteria, DecisionAlternative
            
            # Create decision scenario
            context = DecisionContext(
                decision_type=DecisionType.ANALYTICAL,
                urgency_level=0.6,
                importance_level=0.8
            )
            
            criteria = [
                DecisionCriteria(name="effectiveness", weight=0.4),
                DecisionCriteria(name="feasibility", weight=0.6)
            ]
            
            alternatives = [
                DecisionAlternative(
                    name="Option A",
                    criteria_scores={"effectiveness": 0.8, "feasibility": 0.6}
                ),
                DecisionAlternative(
                    name="Option B", 
                    criteria_scores={"effectiveness": 0.6, "feasibility": 0.9}
                )
            ]
            
            # Make decision
            decision_result = await engine.decision_engine.make_decision(context, criteria, alternatives)
            
            # Validate decision making
            assert "selected_alternative" in decision_result
            assert "confidence" in decision_result
            assert decision_result["confidence"] > 0
            
            self.test_results.append({
                "test": "Decision Making Engine",
                "status": "PASS",
                "details": {
                    "decision_made": True,
                    "confidence": decision_result["confidence"],
                    "alternatives_evaluated": len(alternatives)
                }
            })
            
            print("   ‚úÖ Decision processing: PASS")
            print(f"   ‚úÖ Decision confidence: {decision_result['confidence']:.2f}")
            print(f"   ‚úÖ Alternatives evaluated: {len(alternatives)}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Decision Making Engine",
                "status": "FAIL", 
                "error": str(e)
            })
            print(f"   ‚ùå Decision Making Engine: FAIL - {e}")
    
    async def test_attention_mechanism(self):
        """Test attention management system"""
        
        print("\n7. üéØ Testing Attention Mechanism")
        print("-" * 40)
        
        try:
            engine = CognitiveEngine("test_persona")
            
            # Create attention targets
            targets = [
                AttentionTarget(
                    name="High Priority Task",
                    priority=0.9,
                    cognitive_load=0.5
                ),
                AttentionTarget(
                    name="Routine Task",
                    priority=0.4,
                    cognitive_load=0.3
                )
            ]
            
            # Test attention allocation
            allocation_result = await engine.attention_mechanism.allocate_attention(targets)
            
            # Validate attention system
            assert "allocation" in allocation_result
            assert "cognitive_load" in allocation_result
            assert "attention_efficiency" in allocation_result
            
            self.test_results.append({
                "test": "Attention Mechanism",
                "status": "PASS",
                "details": {
                    "targets_processed": len(targets),
                    "cognitive_load": allocation_result["cognitive_load"],
                    "attention_efficiency": allocation_result["attention_efficiency"]
                }
            })
            
            print("   ‚úÖ Attention allocation: PASS")
            print(f"   ‚úÖ Cognitive load: {allocation_result['cognitive_load']:.2f}")
            print(f"   ‚úÖ Attention efficiency: {allocation_result['attention_efficiency']:.2f}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Attention Mechanism",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Attention Mechanism: FAIL - {e}")
    
    async def test_metacognitive_controller(self):
        """Test metacognitive awareness and control"""
        
        print("\n8. üîç Testing Metacognitive Controller")
        print("-" * 40)
        
        try:
            engine = CognitiveEngine("test_persona")
            
            # Test process monitoring
            process_data = {
                "performance_metrics": {"accuracy": 0.8, "speed": 0.7},
                "resource_usage": {"attention": 0.6, "working_memory": 0.7},
                "duration": 1.5,
                "strategy_used": "analytical"
            }
            
            monitoring_result = await engine.metacognitive_controller.monitor_cognitive_process(
                "test_process", process_data
            )
            
            # Validate metacognitive system
            assert "process_performance" in monitoring_result
            assert "issues_detected" in monitoring_result
            assert "insights" in monitoring_result
            
            self.test_results.append({
                "test": "Metacognitive Controller",
                "status": "PASS",
                "details": {
                    "monitoring_active": True,
                    "issues_detected": len(monitoring_result["issues_detected"]),
                    "insights_generated": len(monitoring_result["insights"])
                }
            })
            
            print("   ‚úÖ Process monitoring: PASS")
            print(f"   ‚úÖ Issues detected: {len(monitoring_result['issues_detected'])}")
            print(f"   ‚úÖ Insights generated: {len(monitoring_result['insights'])}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Metacognitive Controller",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Metacognitive Controller: FAIL - {e}")
    
    async def test_persona_creation(self):
        """Test persona creation and initialization"""
        
        print("\n9. üë§ Testing Persona Creation")
        print("-" * 40)
        
        try:
            # Test different persona types
            persona_types = [
                PersonaType.UNDERGRAD_STUDENT,
                PersonaType.PROFESSOR,
                PersonaType.ACADEMIC_ADVISOR
            ]
            
            created_personas = []
            
            for persona_type in persona_types:
                persona = await self.persona_factory.create_persona_agent(persona_type)
                created_personas.append(persona)
                
                # Validate persona creation
                assert persona is not None
                assert hasattr(persona, 'cognitive_engine')
                assert hasattr(persona, 'persona_type')
            
            self.test_results.append({
                "test": "Persona Creation",
                "status": "PASS",
                "details": {
                    "personas_created": len(created_personas),
                    "persona_types": [p.persona_type.value for p in created_personas]
                }
            })
            
            print("   ‚úÖ Persona creation: PASS")
            print(f"   ‚úÖ Personas created: {len(created_personas)}")
            print(f"   ‚úÖ Types: {', '.join([p.persona_type.value for p in created_personas])}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Persona Creation",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Persona Creation: FAIL - {e}")
    
    async def test_persona_cognitive_profiles(self):
        """Test persona-specific cognitive profiles"""
        
        print("\n10. üß† Testing Persona Cognitive Profiles")
        print("-" * 40)
        
        try:
            # Create different personas and compare profiles
            student = await self.persona_factory.create_persona_agent(PersonaType.UNDERGRAD_STUDENT)
            professor = await self.persona_factory.create_persona_agent(PersonaType.PROFESSOR)
            
            # Get cognitive profiles
            student_attention = student.cognitive_engine.attention_mechanism.attention_params
            professor_attention = professor.cognitive_engine.attention_mechanism.attention_params
            
            # Validate profile differences
            profile_differences = 0
            for param in student_attention:
                if param in professor_attention:
                    if abs(student_attention[param] - professor_attention[param]) > 0.1:
                        profile_differences += 1
            
            self.test_results.append({
                "test": "Persona Cognitive Profiles",
                "status": "PASS",
                "details": {
                    "profiles_generated": True,
                    "profile_differences": profile_differences,
                    "differentiation_successful": profile_differences > 0
                }
            })
            
            print("   ‚úÖ Profile generation: PASS")
            print(f"   ‚úÖ Profile differences detected: {profile_differences}")
            print("   ‚úÖ Persona differentiation: PASS")
            
        except Exception as e:
            self.test_results.append({
                "test": "Persona Cognitive Profiles",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Persona Cognitive Profiles: FAIL - {e}")
    
    async def test_persona_request_processing(self):
        """Test persona-specific request processing"""
        
        print("\n11. üìù Testing Persona Request Processing")
        print("-" * 40)
        
        try:
            # Create personas
            student = await self.persona_factory.create_persona_agent(PersonaType.UNDERGRAD_STUDENT)
            advisor = await self.persona_factory.create_persona_agent(PersonaType.ACADEMIC_ADVISOR)
            
            # Test request
            request = "I'm struggling with time management and need help organizing my study schedule."
            
            # Process with different personas
            student_response = await student.process_request(request)
            advisor_response = await advisor.process_request(request)
            
            # Validate responses
            assert student_response is not None
            assert advisor_response is not None
            assert "confidence" in student_response
            assert "confidence" in advisor_response
            
            self.test_results.append({
                "test": "Persona Request Processing",
                "status": "PASS",
                "details": {
                    "student_response_generated": True,
                    "advisor_response_generated": True,
                    "student_confidence": student_response.get("confidence", 0),
                    "advisor_confidence": advisor_response.get("confidence", 0)
                }
            })
            
            print("   ‚úÖ Student response: PASS")
            print(f"   ‚úÖ Student confidence: {student_response.get('confidence', 0):.2f}")
            print("   ‚úÖ Advisor response: PASS")
            print(f"   ‚úÖ Advisor confidence: {advisor_response.get('confidence', 0):.2f}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Persona Request Processing",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Persona Request Processing: FAIL - {e}")
    
    async def test_multi_agent_initialization(self):
        """Test multi-agent system initialization"""
        
        print("\n12. ü§ñ Testing Multi-Agent System Initialization")
        print("-" * 40)
        
        try:
            # Initialize multi-agent system
            await self.multi_agent_system.initialize()
            
            # Validate initialization
            assert len(self.multi_agent_system.agents) > 0
            assert hasattr(self.multi_agent_system, 'shared_context')
            assert hasattr(self.multi_agent_system, 'coordination_strategies')
            
            self.test_results.append({
                "test": "Multi-Agent Initialization",
                "status": "PASS",
                "details": {
                    "agents_initialized": len(self.multi_agent_system.agents),
                    "shared_memory_active": True,
                    "orchestrator_ready": True
                }
            })
            
            print("   ‚úÖ System initialization: PASS")
            print(f"   ‚úÖ Agents initialized: {len(self.multi_agent_system.agents)}")
            print("   ‚úÖ Shared memory: ACTIVE")
            
        except Exception as e:
            self.test_results.append({
                "test": "Multi-Agent Initialization",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Multi-Agent Initialization: FAIL - {e}")
    
    async def test_agent_collaboration(self):
        """Test agent collaboration capabilities"""
        
        print("\n13. ü§ù Testing Agent Collaboration")
        print("-" * 40)
        
        try:
            # Ensure system is initialized
            if not self.multi_agent_system.agents:
                await self.multi_agent_system.initialize()
            
            # Test agent collaboration
            collaboration_task = {
                "type": "academic_support",
                "description": "Help student with research methodology",
                "complexity": "high",
                "required_expertise": ["research", "methodology", "academic_writing"]
            }
            
            collaboration_result = await self.multi_agent_system.coordinate_task(collaboration_task)
            
            # Validate collaboration
            assert "participating_agents" in collaboration_result
            assert "coordination_strategy" in collaboration_result
            assert len(collaboration_result["participating_agents"]) > 1
            
            self.test_results.append({
                "test": "Agent Collaboration",
                "status": "PASS",
                "details": {
                    "collaboration_successful": True,
                    "participating_agents": len(collaboration_result["participating_agents"]),
                    "coordination_strategy": collaboration_result["coordination_strategy"]
                }
            })
            
            print("   ‚úÖ Agent coordination: PASS")
            print(f"   ‚úÖ Participating agents: {len(collaboration_result['participating_agents'])}")
            print(f"   ‚úÖ Strategy: {collaboration_result['coordination_strategy']}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Agent Collaboration",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Agent Collaboration: FAIL - {e}")
    
    async def test_workflow_orchestration(self):
        """Test workflow orchestration"""
        
        print("\n14. üîÑ Testing Workflow Orchestration")
        print("-" * 40)
        
        try:
            # Ensure system is initialized
            if not self.multi_agent_system.agents:
                await self.multi_agent_system.initialize()
            
            # Test workflow execution
            workflow_request = {
                "workflow_type": "research_support",
                "student_query": "I need help with my thesis research on machine learning applications in education",
                "priority": "high",
                "expected_deliverables": ["research_plan", "methodology_guidance", "resource_recommendations"]
            }
            
            workflow_result = await self.multi_agent_system.coordinate_task(workflow_request)
            
            # Validate workflow execution
            assert "workflow_status" in workflow_result
            assert "results" in workflow_result
            assert workflow_result["workflow_status"] in ["completed", "in_progress"]
            
            self.test_results.append({
                "test": "Workflow Orchestration",
                "status": "PASS",
                "details": {
                    "workflow_executed": True,
                    "status": workflow_result["workflow_status"],
                    "results_generated": len(workflow_result.get("results", []))
                }
            })
            
            print("   ‚úÖ Workflow execution: PASS")
            print(f"   ‚úÖ Status: {workflow_result['workflow_status']}")
            print(f"   ‚úÖ Results generated: {len(workflow_result.get('results', []))}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Workflow Orchestration",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Workflow Orchestration: FAIL - {e}")
    
    async def test_cognitive_integration(self):
        """Test integration of all cognitive systems"""
        
        print("\n15. üîó Testing Cognitive Integration")
        print("-" * 40)
        
        try:
            # Create integrated cognitive scenario
            grad_student = await self.persona_factory.create_persona_agent(PersonaType.GRAD_STUDENT)
            
            # Complex integrated request
            complex_request = {
                "text": "I'm working on my dissertation and facing multiple challenges: my data analysis isn't working as expected, I'm behind schedule, my advisor seems concerned, and I'm feeling overwhelmed. I need comprehensive support to get back on track.",
                "context": {
                    "academic_level": "graduate",
                    "domain": "research",
                    "issues": ["technical", "time_management", "emotional", "advisory"],
                    "urgency": "high"
                }
            }
            
            # Process through integrated pipeline
            start_time = datetime.now()
            result = await grad_student.process_request(complex_request["text"])
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Validate integration
            assert result is not None
            assert "confidence" in result
            assert "support_assessment" in result
            assert processing_time < 5.0  # Should complete within 5 seconds
            
            self.test_results.append({
                "test": "Cognitive Integration",
                "status": "PASS",
                "details": {
                    "integration_successful": True,
                    "processing_time": processing_time,
                    "confidence": result["confidence"],
                    "support_types_identified": len(result.get("support_assessment", {}).get("support_types", []))
                }
            })
            
            print("   ‚úÖ Integrated processing: PASS")
            print(f"   ‚úÖ Processing time: {processing_time:.2f}s")
            print(f"   ‚úÖ Confidence: {result['confidence']:.2f}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Cognitive Integration",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Cognitive Integration: FAIL - {e}")
    
    async def test_end_to_end_scenarios(self):
        """Test complete end-to-end scenarios"""
        
        print("\n16. üéØ Testing End-to-End Scenarios")
        print("-" * 40)
        
        try:
            scenarios_passed = 0
            total_scenarios = 3
            
            # Scenario 1: Undergraduate academic support
            undergrad = await self.persona_factory.create_persona_agent(PersonaType.UNDERGRAD_STUDENT)
            undergrad_request = "I'm struggling in my calculus class and need help understanding derivatives"
            undergrad_result = await undergrad.process_request(undergrad_request)
            
            if undergrad_result and undergrad_result.get("confidence", 0) > 0.5:
                scenarios_passed += 1
                print("   ‚úÖ Undergraduate scenario: PASS")
            else:
                print("   ‚ùå Undergraduate scenario: FAIL")
            
            # Scenario 2: Graduate research support
            grad = await self.persona_factory.create_persona_agent(PersonaType.GRAD_STUDENT)
            grad_request = "I need help with my thesis proposal and methodology design"
            grad_result = await grad.process_request(grad_request)
            
            if grad_result and grad_result.get("confidence", 0) > 0.5:
                scenarios_passed += 1
                print("   ‚úÖ Graduate scenario: PASS")
            else:
                print("   ‚ùå Graduate scenario: FAIL")
            
            # Scenario 3: Faculty support
            faculty = await self.persona_factory.create_persona_agent(PersonaType.PROFESSOR)
            faculty_request = "I'm developing a new course curriculum and need pedagogical guidance"
            faculty_result = await faculty.process_request(faculty_request)
            
            if faculty_result and faculty_result.get("confidence", 0) > 0.5:
                scenarios_passed += 1
                print("   ‚úÖ Faculty scenario: PASS")
            else:
                print("   ‚ùå Faculty scenario: FAIL")
            
            self.test_results.append({
                "test": "End-to-End Scenarios",
                "status": "PASS" if scenarios_passed >= 2 else "FAIL",
                "details": {
                    "scenarios_passed": scenarios_passed,
                    "total_scenarios": total_scenarios,
                    "success_rate": scenarios_passed / total_scenarios
                }
            })
            
            print(f"   üìä Scenarios passed: {scenarios_passed}/{total_scenarios}")
            
        except Exception as e:
            self.test_results.append({
                "test": "End-to-End Scenarios",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå End-to-End Scenarios: FAIL - {e}")
    
    async def test_performance_benchmarks(self):
        """Test performance benchmarks"""
        
        print("\n17. ‚ö° Testing Performance Benchmarks")
        print("-" * 40)
        
        try:
            # Performance test parameters
            num_requests = 10
            max_response_time = 3.0  # seconds
            min_confidence = 0.5
            
            persona = await self.persona_factory.create_persona_agent(PersonaType.UNDERGRAD_STUDENT)
            
            response_times = []
            confidences = []
            
            # Run performance tests
            for i in range(num_requests):
                start_time = datetime.now()
                
                test_request = f"Test request {i+1}: I need help with my studies"
                result = await persona.process_request(test_request)
                
                response_time = (datetime.now() - start_time).total_seconds()
                response_times.append(response_time)
                
                if result and "confidence" in result:
                    confidences.append(result["confidence"])
                else:
                    confidences.append(0.0)
            
            # Calculate performance metrics
            avg_response_time = sum(response_times) / len(response_times)
            avg_confidence = sum(confidences) / len(confidences)
            max_response_time_actual = max(response_times)
            
            performance_pass = (
                avg_response_time <= max_response_time and
                avg_confidence >= min_confidence and
                max_response_time_actual <= max_response_time * 1.5
            )
            
            self.test_results.append({
                "test": "Performance Benchmarks",
                "status": "PASS" if performance_pass else "FAIL",
                "details": {
                    "avg_response_time": avg_response_time,
                    "max_response_time": max_response_time_actual,
                    "avg_confidence": avg_confidence,
                    "requests_processed": num_requests
                }
            })
            
            print(f"   ‚úÖ Requests processed: {num_requests}")
            print(f"   ‚úÖ Avg response time: {avg_response_time:.2f}s")
            print(f"   ‚úÖ Max response time: {max_response_time_actual:.2f}s")
            print(f"   ‚úÖ Avg confidence: {avg_confidence:.2f}")
            print(f"   {'‚úÖ' if performance_pass else '‚ùå'} Performance benchmark: {'PASS' if performance_pass else 'FAIL'}")
            
        except Exception as e:
            self.test_results.append({
                "test": "Performance Benchmarks",
                "status": "FAIL",
                "error": str(e)
            })
            print(f"   ‚ùå Performance Benchmarks: FAIL - {e}")
    
    def generate_test_report(self):
        """Generate comprehensive test report"""
        
        print("\n" + "="*60)
        print("üß™ COMPREHENSIVE TEST REPORT")
        print("="*60)
        
        # Calculate summary statistics
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result["status"] == "PASS")
        failed_tests = total_tests - passed_tests
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        print(f"\nüìä SUMMARY STATISTICS:")
        print(f"   ‚Ä¢ Total Tests Run: {total_tests}")
        print(f"   ‚Ä¢ Tests Passed: {passed_tests}")
        print(f"   ‚Ä¢ Tests Failed: {failed_tests}")
        print(f"   ‚Ä¢ Success Rate: {success_rate:.1f}%")
        
        # Category breakdown
        categories = {
            "Cognitive Architecture": [
                "Cognitive Core Engine", "Perception System", "Reasoning Engine",
                "Memory System", "Learning System", "Decision Making Engine",
                "Attention Mechanism", "Metacognitive Controller"
            ],
            "Persona System": [
                "Persona Creation", "Persona Cognitive Profiles", "Persona Request Processing"
            ],
            "Multi-Agent System": [
                "Multi-Agent Initialization", "Agent Collaboration", "Workflow Orchestration"
            ],
            "Integration": [
                "Cognitive Integration", "End-to-End Scenarios", "Performance Benchmarks"
            ]
        }
        
        print(f"\nüìã CATEGORY BREAKDOWN:")
        for category, test_names in categories.items():
            category_results = [r for r in self.test_results if r["test"] in test_names]
            category_passed = sum(1 for r in category_results if r["status"] == "PASS")
            category_total = len(category_results)
            category_rate = (category_passed / category_total) * 100 if category_total > 0 else 0
            
            print(f"   ‚Ä¢ {category}: {category_passed}/{category_total} ({category_rate:.1f}%)")
        
        # Detailed results
        print(f"\nüìù DETAILED RESULTS:")
        for result in self.test_results:
            status_icon = "‚úÖ" if result["status"] == "PASS" else "‚ùå"
            print(f"   {status_icon} {result['test']}: {result['status']}")
            
            if result["status"] == "FAIL" and "error" in result:
                print(f"      Error: {result['error']}")
            elif "details" in result:
                details = result["details"]
                key_metrics = []
                for key, value in details.items():
                    if isinstance(value, float):
                        key_metrics.append(f"{key}: {value:.2f}")
                    elif isinstance(value, (int, bool, str)) and len(str(value)) < 30:
                        key_metrics.append(f"{key}: {value}")
                
                if key_metrics:
                    print(f"      {', '.join(key_metrics[:3])}")
        
        # System health assessment
        print(f"\nüè• SYSTEM HEALTH ASSESSMENT:")
        
        core_cognitive_tests = [
            "Cognitive Core Engine", "Perception System", "Reasoning Engine", "Memory System"
        ]
        core_passed = sum(1 for r in self.test_results 
                         if r["test"] in core_cognitive_tests and r["status"] == "PASS")
        core_health = (core_passed / len(core_cognitive_tests)) * 100
        
        persona_tests = ["Persona Creation", "Persona Cognitive Profiles", "Persona Request Processing"]
        persona_passed = sum(1 for r in self.test_results 
                           if r["test"] in persona_tests and r["status"] == "PASS")
        persona_health = (persona_passed / len(persona_tests)) * 100
        
        integration_tests = ["Cognitive Integration", "End-to-End Scenarios"]
        integration_passed = sum(1 for r in self.test_results 
                               if r["test"] in integration_tests and r["status"] == "PASS")
        integration_health = (integration_passed / len(integration_tests)) * 100
        
        print(f"   ‚Ä¢ Core Cognitive Systems: {core_health:.1f}% healthy")
        print(f"   ‚Ä¢ Persona System: {persona_health:.1f}% healthy")
        print(f"   ‚Ä¢ System Integration: {integration_health:.1f}% healthy")
        
        # Overall system status
        overall_health = (core_health + persona_health + integration_health) / 3
        
        if overall_health >= 90:
            health_status = "üü¢ EXCELLENT"
        elif overall_health >= 75:
            health_status = "üü° GOOD"
        elif overall_health >= 60:
            health_status = "üü† FAIR"
        else:
            health_status = "üî¥ NEEDS ATTENTION"
        
        print(f"\nüéØ OVERALL SYSTEM STATUS: {health_status} ({overall_health:.1f}%)")
        
        # Recommendations
        print(f"\nüí° RECOMMENDATIONS:")
        if failed_tests == 0:
            print("   ‚Ä¢ System is operating optimally")
            print("   ‚Ä¢ Ready for production deployment")
            print("   ‚Ä¢ Consider performance optimization")
        elif failed_tests <= 2:
            print("   ‚Ä¢ Address minor test failures")
            print("   ‚Ä¢ System is largely functional")
            print("   ‚Ä¢ Monitor performance in production")
        else:
            print("   ‚Ä¢ Address critical test failures before deployment")
            print("   ‚Ä¢ Review system architecture")
            print("   ‚Ä¢ Implement additional error handling")
        
        # Save report to file
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "success_rate": success_rate
            },
            "test_results": self.test_results,
            "system_health": {
                "core_cognitive": core_health,
                "persona_system": persona_health,
                "integration": integration_health,
                "overall": overall_health
            }
        }
        
        # Create reports directory if it doesn't exist
        reports_dir = Path("reports")
        reports_dir.mkdir(exist_ok=True)
        
        # Save detailed report
        report_file = reports_dir / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)
        
        print(f"\nüìÑ Detailed report saved to: {report_file}")
        print("="*60)


# Run the comprehensive test suite
async def main():
    """Run the comprehensive CollegiumAI test suite"""
    
    print("üöÄ Starting CollegiumAI Comprehensive Test Suite...")
    print("This may take a few minutes to complete all tests.\n")
    
    # Initialize and run test suite
    test_suite = CollegiumAITestSuite()
    await test_suite.run_all_tests()
    
    print("\nüéâ Test suite completed!")
    print("Check the generated report for detailed results.")


if __name__ == "__main__":
    # Set up asyncio event loop and run tests
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Test suite interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Test suite failed with error: {e}")
        import traceback
        traceback.print_exc()